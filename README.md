ğŸš€ Employee Data Pipeline using Snowflake (Streams + Tasks)
ğŸ“Œ Project Overview

This project demonstrates a real-time Change Data Capture (CDC) pipeline in Snowflake using Streams and Tasks.
It captures incremental changes (INSERT/UPDATE/DELETE) from a source employee table, merges them into a central target table, and then distributes records into department-specific tables (Sales, IT, Finance) using a DAG-like flow.

ğŸ—ï¸ Architecture Workflow

Source Table (emp_source) â Stream (emp_source_stream) â Target Table (emp_target) â Streams on Target â Departmental Tables (emp_Sales, emp_IT, emp_Finance)

âœ… All data movement is automated via Snowflake Tasks scheduled every minute.

ğŸ”‘ Key Features

Change Data Capture (CDC): Implemented using Streams to track only incremental changes.

Incremental Load with MERGE: Updates, inserts, and deletes handled using metadata$action and metadata$isupdate.

Task Automation: Tasks run every 1 minute to merge changes automatically.

Departmental Data Mart Creation: Data distributed into department-specific tables (Sales, IT, Finance).

DAG Flow Simulation: End-to-end pipeline from Source â†’ ODS â†’ Departmental marts.

ğŸ—„ï¸ Tables Created

emp_source â€“ Source table with employee data.

emp_target â€“ Central target table (ODS/Staging layer).

emp_Sales, emp_IT, emp_Finance â€“ Department-level downstream tables.

âš¡ Snowflake Objects Used

Tables: Source, Target, Departmental

Streams: Track changes in source & target tables

Tasks: Automate merge operations every 1 minute

Warehouse: tasks_wh used for task execution

ğŸ“Š Example Use Case

HR inserts/updates employee records in emp_source.

Pipeline automatically updates emp_target.

Department-level tables (emp_Sales, emp_IT, emp_Finance) always stay in sync.

Useful for real-time analytics, departmental reporting, and data warehouse pipelines.

ğŸ§‘â€ğŸ’» Skills Highlighted

Snowflake Data Engineering

Streams & Tasks (CDC)

SQL (DML + MERGE)

Incremental Data Pipelines

Data Marts Creation

DAG Workflow Design



ğŸ“Œ How to Run

Create emp_source table and insert records.

Create streams and target table (emp_target).

Setup department-specific tables (Sales, IT, Finance).

Create and resume tasks (emp_task, emp_sales_task, emp_IT_task, emp_finance_task).

Perform DML changes on emp_source and watch changes flow through the pipeline automatically.



ğŸŒŸ Project Impact

This project highlights real-time data engineering skills and demonstrates how to:

Capture incremental data using Streams.

Automate ETL workflows with Tasks.

Build a multi-layered data pipeline in Snowflake.

Apply CDC concepts in practical scenarios.
